{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7494445,"sourceType":"datasetVersion","datasetId":4363827}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-29T14:32:59.374833Z","iopub.execute_input":"2024-01-29T14:32:59.375794Z","iopub.status.idle":"2024-01-29T14:32:59.885456Z","shell.execute_reply.started":"2024-01-29T14:32:59.375731Z","shell.execute_reply":"2024-01-29T14:32:59.884312Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/pubdata/acm_1995_2004.csv\n/kaggle/input/pubdata/dblp_1995_2004.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyspark ","metadata":{"execution":{"iopub.status.busy":"2024-01-29T14:33:16.361832Z","iopub.execute_input":"2024-01-29T14:33:16.362487Z","iopub.status.idle":"2024-01-29T14:34:11.461186Z","shell.execute_reply.started":"2024-01-29T14:33:16.362444Z","shell.execute_reply":"2024-01-29T14:34:11.459997Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=76e5ce43386a9fe273179301fc757858c3f1be537a468526d65f52c404bb6e2c\n  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install python-levenshtein","metadata":{"execution":{"iopub.status.busy":"2024-01-29T14:34:15.063475Z","iopub.execute_input":"2024-01-29T14:34:15.063940Z","iopub.status.idle":"2024-01-29T14:34:29.692593Z","shell.execute_reply.started":"2024-01-29T14:34:15.063895Z","shell.execute_reply":"2024-01-29T14:34:29.691030Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: python-levenshtein in /opt/conda/lib/python3.10/site-packages (0.23.0)\nRequirement already satisfied: Levenshtein==0.23.0 in /opt/conda/lib/python3.10/site-packages (from python-levenshtein) (0.23.0)\nRequirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from Levenshtein==0.23.0->python-levenshtein) (3.6.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.session import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\nfrom pyspark.sql.functions import split\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql.functions import col, concat_ws\nfrom pyspark.sql.functions import regexp_replace, regexp_extract, collect_list, explode, udf\nfrom pyspark.sql.types import DoubleType\nfrom Levenshtein import distance, ratio","metadata":{"execution":{"iopub.status.busy":"2024-01-29T14:34:29.695413Z","iopub.execute_input":"2024-01-29T14:34:29.695845Z","iopub.status.idle":"2024-01-29T14:34:29.912292Z","shell.execute_reply.started":"2024-01-29T14:34:29.695807Z","shell.execute_reply":"2024-01-29T14:34:29.910917Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"spark = SparkSession.builder.appName(\"pubdata\").getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2024-01-29T14:34:33.492596Z","iopub.execute_input":"2024-01-29T14:34:33.495113Z","iopub.status.idle":"2024-01-29T14:34:39.780557Z","shell.execute_reply.started":"2024-01-29T14:34:33.495057Z","shell.execute_reply":"2024-01-29T14:34:39.779318Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/01/29 14:34:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the schema\npub_schema = StructType([\n    StructField(\"title\", StringType(), True),\n    StructField(\"authors\", StringType(), True),\n    StructField(\"year\", StringType(), True),\n    StructField(\"journal\", StringType(), True),\n    StructField(\"index\", StringType(), True),\n])","metadata":{"execution":{"iopub.status.busy":"2024-01-29T14:34:40.582833Z","iopub.execute_input":"2024-01-29T14:34:40.583316Z","iopub.status.idle":"2024-01-29T14:34:40.591260Z","shell.execute_reply.started":"2024-01-29T14:34:40.583276Z","shell.execute_reply":"2024-01-29T14:34:40.589657Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def create_dataframe_from_file(file_path, pub_schema):\n    # Define the custom delimiter\n    delimiter = \"\\n\\n\"\n\n    # Create an RDD using newAPIHadoopFile with TextInputFormat\n    rdd = spark.sparkContext.newAPIHadoopFile(\n        file_path,\n        \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\",\n        \"org.apache.hadoop.io.LongWritable\",\n        \"org.apache.hadoop.io.Text\",\n        conf={\"textinputformat.record.delimiter\": delimiter},\n    )\n\n    # Filter and map the RDD to extract relevant fields\n    data_rdd = rdd.filter(lambda x: not x[1].startswith('#%')).map(lambda x: tuple(\n        (\n            next((field[2:] for field in x[1].splitlines() if field.startswith('#*')), None),  # Paper Title\n            next((field[2:] for field in x[1].splitlines() if field.startswith('#@')), None),  # Authors\n            next((field[2:] for field in x[1].splitlines() if field.startswith('#t')), None),  # Year\n            next((field[2:] for field in x[1].splitlines() if field.startswith('#c')), None),  # Publication Venue\n            next((field[6:] for field in x[1].splitlines() if field.startswith('#index')), None)  # Index ID\n        )\n    ))\n\n    # Create DataFrame using the defined schema\n    df = spark.createDataFrame(data_rdd, schema=pub_schema)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-01-27T19:48:10.413642Z","iopub.execute_input":"2024-01-27T19:48:10.414167Z","iopub.status.idle":"2024-01-27T19:48:19.062955Z","shell.execute_reply.started":"2024-01-27T19:48:10.414129Z","shell.execute_reply":"2024-01-27T19:48:19.061473Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+-----+-------+----+-------+-----+\n|title|authors|year|journal|index|\n+-----+-------+----+-------+-----+\n|NULL |NULL   |NULL|NULL   |NULL |\n+-----+-------+----+-------+-----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def read_csv_with_schema(spark, file_path, schema):\n    \"\"\"\n    Read records from a CSV file using a specified schema.\n\n    Args:\n    - spark: SparkSession object\n    - file_path: path to the CSV file\n    - schema: schema to be applied to the DataFrame\n\n    Returns:\n    - DataFrame containing the records from the CSV file with the specified schema\n    \"\"\"\n    # Read CSV file with schema\n    df = spark.read.csv(file_path, schema=schema, header=True)\n\n    return df\n\nfile_path_acm = \"/kaggle/input/pubdata/acm_1995_2004.csv\"\nfile_path_dblp = \"/kaggle/input/pubdata/dblp_1995_2004.csv\"\n\n# Read CSV file with schema\ndf_acm = read_csv_with_schema(spark, file_path_acm, pub_schema)\ndf_dblp = read_csv_with_schema(spark, file_path_dblp, pub_schema)\n\n# Show the DataFrame\ndf_acm.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-29T14:34:53.986810Z","iopub.execute_input":"2024-01-29T14:34:53.987317Z","iopub.status.idle":"2024-01-29T14:35:01.123234Z","shell.execute_reply.started":"2024-01-29T14:34:53.987280Z","shell.execute_reply":"2024-01-29T14:35:01.122211Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+--------------------+--------------------+----+-------+--------------------+\n|               title|             authors|year|journal|               index|\n+--------------------+--------------------+----+-------+--------------------+\n|The next database...|            Jim Gray|2004| SIGMOD|5390972920f70186a...|\n|The role of crypt...|         Ueli Maurer|2004| SIGMOD|5390972920f70186a...|\n|Tree logical clas...|Stelios Paparizos...|2004| SIGMOD|5390972920f70186a...|\n|Adaptive stream r...|Ankur Jain, Edwar...|2004| SIGMOD|5390972920f70186a...|\n|Holistic UDAFs at...|Graham Cormode, T...|2004| SIGMOD|5390972920f70186a...|\n|Online eventdrive...|Huanmei Wu, Betty...|2004| SIGMOD|5390972920f70186a...|\n|Using the structu...|Kristina Lerman, ...|2004| SIGMOD|5390972920f70186a...|\n|FleXPath flexible...|Sihem AmerYahia, ...|2004| SIGMOD|5390972920f70186a...|\n|An interactive cl...|Wensheng Wu, Clem...|2004| SIGMOD|5390972920f70186a...|\n|Lazy query evalua...|Serge Abiteboul, ...|2004| SIGMOD|5390972920f70186a...|\n|Colorful XML one ...|H V Jagadish, Lak...|2004| SIGMOD|5390972920f70186a...|\n|Effective use of ...|Surajit Chaudhuri...|2004| SIGMOD|5390972920f70186a...|\n|The Priority Rtre...|Lars Arge, Mark d...|2004| SIGMOD|5390972920f70186a...|\n|Online maintenanc...|Christopher Jerma...|2004| SIGMOD|5390972920f70186a...|\n|Integrating verti...|Sanjay Agrawal, V...|2004| SIGMOD|5390972920f70186a...|\n|Graph indexing a ...|Xifeng Yan, Phili...|2004| SIGMOD|5390972920f70186a...|\n|Conditional selec...|Nicolas Bruno, Su...|2004| SIGMOD|5390972920f70186a...|\n|Computing Cluster...|Christian Bhm, Ka...|2004| SIGMOD|5390972920f70186a...|\n|Transaction suppo...|Goetz Graefe, Mic...|2004| SIGMOD|5390972920f70186a...|\n|Fast computation ...|Naga K Govindaraj...|2004| SIGMOD|5390972920f70186a...|\n+--------------------+--------------------+----+-------+--------------------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df_dblp.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-29T14:35:05.251816Z","iopub.execute_input":"2024-01-29T14:35:05.252273Z","iopub.status.idle":"2024-01-29T14:35:05.588169Z","shell.execute_reply.started":"2024-01-29T14:35:05.252240Z","shell.execute_reply":"2024-01-29T14:35:05.586744Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"+--------------------+--------------------+----+-------+--------------------+\n|               title|             authors|year|journal|               index|\n+--------------------+--------------------+----+-------+--------------------+\n|An initial study ...|      Amol Deshpande|2004| SIGMOD|53e9a515b7602d970...|\n|Engineering Feder...|Stefan Conrad, Wi...|1999| SIGMOD|53e9b275b7602d970...|\n|Information Findi...|Tak W Yan, Hector...|1995| SIGMOD|53e9a5beb7602d970...|\n|       Editors Notes|      Jennifer Widom|1995| SIGMOD|53e99800b7602d970...|\n|Report on the 5th...|HansJoachim Lenz,...|2003| SIGMOD|53e9a718b7602d970...|\n|       Editors Notes|            Ling Liu|2002| SIGMOD|53e99800b7602d970...|\n|Report from the N...|Amit P Sheth, Dim...|1996| SIGMOD|53e99e6ab7602d970...|\n|TODS Perceptions ...| Richard T Snodgrass|2002| SIGMOD|53e99ae6b7602d970...|\n|SQLMED  A Status ...|Jim Melton, JanEi...|2002| SIGMOD|53e9b4d4b7602d970...|\n|2003 SIGMOD Innov...| Donald D Chamberlin|2003| SIGMOD|53e9b4a5b7602d970...|\n|Jeffrey D Ullman ...|   Marianne Winslett|2001| SIGMOD|53e9aadfb7602d970...|\n|Research issues f...|Leslie D Fife, Le...|2003| SIGMOD|53e9a7b3b7602d970...|\n|      Chairs Message| Richard T Snodgrass|1998| SIGMOD|53e99809b7602d970...|\n|An approach to co...|Debajyoti Mukhopa...|2003| SIGMOD|53e9a515b7602d970...|\n|A Comparison of T...|Carl Medsker, Mar...|1995| SIGMOD|53e9bd1eb7602d970...|\n|Object Query Stan...|       Andrew E Wade|1996| SIGMOD|53e9983db7602d970...|\n|Constraints for S...|Peter Buneman, We...|2001| SIGMOD|53e9b991b7602d970...|\n|Database Principl...|       Leonid Libkin|1999| SIGMOD|53e99a26b7602d970...|\n|Report on DART 96...|Krithi Ramamritha...|1997| SIGMOD|53e9a743b7602d970...|\n|A MultiAgent Syst...|Riza Cenk Erdur, ...|2002| SIGMOD|53e9bd8cb7602d970...|\n+--------------------+--------------------+----+-------+--------------------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def filter_and_clean_df(df):\n    # Filter publications between 1995 and 2004 in VLDB and SIGMOD venues\n    filtered_df = df.filter(\n        (col(\"year\").cast(\"int\").between(1995, 2004)) &\n        (col(\"journal\").rlike(\"(?i)SIGMOD|VLDB\"))\n    )\n\n    # Clean the journal column\n    cleaned_df = filtered_df.withColumn(\"journal\",\n                                        regexp_replace(\n                                            regexp_replace(\"journal\", \"(?i).*\\\\bVLDB\\\\b.*\", \"VLDB\"),\n                                            \"(?i).*\\\\bSIGMOD\\\\b.*\", \"SIGMOD\"))\n\n    return cleaned_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_special_chars(df):\n  pattern = r'[^\\w,\\s]'\n  return df.withColumn(\"title\", regexp_replace(\"title\", pattern,'')).withColumn(\"authors\", regexp_replace(\"authors\", pattern,''))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_to_csv(dataframe, path):\n    dataframe.repartition(1).write.format('csv').option(\"header\", \"true\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").save(path, mode='overwrite')","metadata":{"execution":{"iopub.status.busy":"2024-01-29T14:35:16.442078Z","iopub.execute_input":"2024-01-29T14:35:16.442550Z","iopub.status.idle":"2024-01-29T14:35:16.449646Z","shell.execute_reply.started":"2024-01-29T14:35:16.442505Z","shell.execute_reply":"2024-01-29T14:35:16.448350Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lower, count\nfrom pyspark.sql.window import Window\n\ndef find_duplicates(df):\n    # Convert titles, authors, and year to lowercase for case-insensitive comparison\n    df = df.withColumn(\"lower_title\", lower(col(\"title\"))) \\\n           .withColumn(\"lower_authors\", lower(col(\"authors\"))) \n    \n    # Find duplicates using window function\n    windowSpec = Window.partitionBy(\"lower_title\", \"lower_authors\", \"year\", \"index\")\n    df_with_count = df.withColumn(\"count\", count(\"*\").over(windowSpec))\n    \n    # Filter only rows with count > 1, indicating duplicates\n    duplicates_df = df_with_count.filter(col(\"count\") > 1).orderBy(col(\"count\").desc()) \\\n                                 .drop(\"lower_title\", \"lower_authors\")\n    \n    # Remove duplicates from original DataFrame\n    df_unique = df_with_count.filter(col(\"count\") == 1).drop(\"count\", \"lower_title\", \"lower_authors\")\n    \n    return duplicates_df, df_unique\n\nduplicates, unique_records = find_duplicates(df_acm)\n\nduplicates.show(truncate=False)\n#unique_records.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def levenshtein_ratio(s1, s2):\n    return ratio(s1, s2)\n\n# Register the UDF to calculate Levenshtein ratio\nlevenshtein_ratio_udf = udf(levenshtein_ratio, DoubleType())\n\ndef find_duplicates_for_ground_truth_lv(df1, df2, threshold=0.5):\n    # Select only the \"title\" and \"authors\" columns from df1 and alias them\n    df1_subset = df1.select(col(\"title\").alias(\"title_1\"), col(\"authors\").alias(\"authors_1\"))\n\n    # Select only the \"title\" and \"authors\" columns from df2 and alias them\n    df2_subset = df2.select(col(\"title\").alias(\"title_2\"), col(\"authors\").alias(\"authors_2\"))\n\n    # Join the subsets of df1 and df2 containing \"title\" and \"authors\" columns\n    joined_df = df1_subset.crossJoin(df2_subset)\n\n    # Calculate the similarity score using Levenshtein ratio on \"title\" and \"authors\"\n    similarity_df = joined_df.withColumn(\"Title_Similarity\", levenshtein_ratio_udf(joined_df[\"title_1\"], joined_df[\"title_2\"])) \\\n                             .withColumn(\"Authors_Similarity\", levenshtein_ratio_udf(joined_df[\"authors_1\"], joined_df[\"authors_2\"]))\n\n    # Filter the DataFrame to keep only the pairs with similarity scores greater than or equal to the specified threshold\n    duplicates_df = similarity_df.filter((similarity_df[\"Title_Similarity\"] >= threshold) & (similarity_df[\"Authors_Similarity\"] >= threshold))\n\n    # Select and rename the relevant columns\n    duplicates_df = duplicates_df.select(duplicates_df[\"title_1\"].alias(\"Title1\"),\n                                         duplicates_df[\"title_2\"].alias(\"Title2\"),\n                                         duplicates_df[\"authors_1\"].alias(\"Authors1\"),\n                                         duplicates_df[\"authors_2\"].alias(\"Authors2\"),\n                                         duplicates_df[\"Title_Similarity\"],\n                                         duplicates_df[\"Authors_Similarity\"])\n\n    return duplicates_df\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:36:24.688896Z","iopub.execute_input":"2024-01-27T20:36:24.689378Z","iopub.status.idle":"2024-01-27T20:36:24.703914Z","shell.execute_reply.started":"2024-01-27T20:36:24.689344Z","shell.execute_reply":"2024-01-27T20:36:24.702379Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def jaccard_similarity_case_insensitive(str1, str2):\n    if str1 is None or str2 is None:\n        return 0.0\n    set1 = set(str1.lower().split())\n    set2 = set(str2.lower().split())\n\n    intersection = len(set1.intersection(set2))\n    union = len(set1.union(set2))\n\n    similarity = intersection / union if union != 0 else 0\n    return similarity","metadata":{"execution":{"iopub.status.busy":"2024-01-29T14:37:56.839645Z","iopub.execute_input":"2024-01-29T14:37:56.840182Z","iopub.status.idle":"2024-01-29T14:37:56.848338Z","shell.execute_reply.started":"2024-01-29T14:37:56.840143Z","shell.execute_reply":"2024-01-29T14:37:56.846815Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Register the UDF to calculate Jaccard similarity\njaccard_similarity_udf = udf(jaccard_similarity_case_insensitive, DoubleType())\n\ndef find_duplicates_for_ground_truth_js(df1, df2, threshold=0.5):\n    # Select only the \"title\" and \"authors\" columns from df1 and alias them\n    df1_subset = df1.select(col(\"title\").alias(\"title_1\"), col(\"authors\").alias(\"authors_1\"))\n\n    # Select only the \"title\" and \"authors\" columns from df2 and alias them\n    df2_subset = df2.select(col(\"title\").alias(\"title_2\"), col(\"authors\").alias(\"authors_2\"))\n\n    # Join the subsets of df1 and df2 containing \"title\" and \"authors\" columns\n    joined_df = df1_subset.crossJoin(df2_subset)\n\n    # Calculate the similarity score using Jaccard similarity on \"title\" and \"authors\"\n    similarity_df = joined_df.withColumn(\"Title_Similarity\", jaccard_similarity_udf(joined_df[\"title_1\"], joined_df[\"title_2\"])) \\\n                             .withColumn(\"Authors_Similarity\", jaccard_similarity_udf(joined_df[\"authors_1\"], joined_df[\"authors_2\"]))\n\n    # Filter the DataFrame to keep only the pairs with similarity scores greater than or equal to the specified threshold\n    duplicates_df = similarity_df.filter((similarity_df[\"Title_Similarity\"] >= threshold) & (similarity_df[\"Authors_Similarity\"] >= threshold))\n\n    # Select and rename the relevant columns\n    duplicates_df = duplicates_df.select(duplicates_df[\"title_1\"].alias(\"Title1\"),\n                                         duplicates_df[\"title_2\"].alias(\"Title2\"),\n                                         duplicates_df[\"authors_1\"].alias(\"Authors1\"),\n                                         duplicates_df[\"authors_2\"].alias(\"Authors2\"),\n                                         duplicates_df[\"Title_Similarity\"],\n                                         duplicates_df[\"Authors_Similarity\"])\n\n    return duplicates_df","metadata":{"execution":{"iopub.status.busy":"2024-01-29T14:38:03.995307Z","iopub.execute_input":"2024-01-29T14:38:03.995788Z","iopub.status.idle":"2024-01-29T14:38:04.008369Z","shell.execute_reply.started":"2024-01-29T14:38:03.995754Z","shell.execute_reply":"2024-01-29T14:38:04.007096Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_dblp = create_dataframe_from_file(file_path_dblp, pub_schema)\ndf_acm = create_dataframe_from_file(file_path_acm, pub_schema)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_dblp_clean = remove_special_chars(filter_and_clean_df(df_dblp))\ndf_acm_clean = remove_special_chars(filter_and_clean_df(df_acm))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"duplicates_find_naive = find_duplicates_for_ground_truth_js(df_acm, df_dblp)","metadata":{"execution":{"iopub.status.busy":"2024-01-29T14:38:12.336179Z","iopub.execute_input":"2024-01-29T14:38:12.336642Z","iopub.status.idle":"2024-01-29T14:38:12.484542Z","shell.execute_reply.started":"2024-01-29T14:38:12.336606Z","shell.execute_reply":"2024-01-29T14:38:12.483175Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/","metadata":{"execution":{"iopub.status.busy":"2024-01-29T14:36:28.103603Z","iopub.execute_input":"2024-01-29T14:36:28.104042Z","iopub.status.idle":"2024-01-29T14:36:29.214949Z","shell.execute_reply.started":"2024-01-29T14:36:28.104009Z","shell.execute_reply":"2024-01-29T14:36:29.213634Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"input  lib  working\n","output_type":"stream"}]},{"cell_type":"code","source":"write_to_csv(duplicates_find_naive, \"dublicates_js_1995_2004.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-01-29T14:38:15.289163Z","iopub.execute_input":"2024-01-29T14:38:15.289635Z","iopub.status.idle":"2024-01-29T14:39:57.260913Z","shell.execute_reply.started":"2024-01-29T14:38:15.289596Z","shell.execute_reply":"2024-01-29T14:39:57.259637Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"24/01/29 14:38:15 WARN ExtractPythonUDFFromJoinCondition: The join condition:((jaccard_similarity_case_insensitive(title_1#125, title_2#129)#137 >= 0.5) AND (jaccard_similarity_case_insensitive(authors_1#126, authors_2#130)#144 >= 0.5)) of the join plan contains PythonUDF only, it will be moved out and the join plan will be turned to cross join.\n                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}